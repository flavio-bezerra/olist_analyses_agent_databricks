{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Debug Dedicado: Context Manager\n",
                "\n",
                "Este notebook foi criado para isolar e debugar falhas no `ContextManager`, focando em:\n",
                "1. **Conectividade**: Validar se conseguimos listar tabelas e colunas via Spark.\n",
                "2. **Segurança/Permissão**: Verificar se métodos como `.toPandas()` funcionam sem erros de bloqueio (WhiteList/Socket).\n",
                "3. **Formatação de Prompt**: Garantir que a string de contexto gerada esteja correta e legível para o LLM."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%load_ext autoreload\n",
                "%autoreload 2\n",
                "import os\n",
                "import sys\n",
                "from pyspark.sql import SparkSession\n",
                "\n",
                "# Adiciona o diretório atual ao path para importação dos módulos\n",
                "sys.path.append(os.getcwd())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Inicialização da Sessão Spark (Segura)\n",
                "Tentamos pegar a sessão ativa para evitar conflitos."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    spark = SparkSession.getActiveSession()\n",
                "    if not spark:\n",
                "        print(\"Criando nova sessão Spark (Ambiente Local)...\")\n",
                "        spark = SparkSession.builder.appName(\"DebugContextManager\").getOrCreate()\n",
                "    print(f\"[OK] Spark Session: {spark}\")\n",
                "except Exception as e:\n",
                "    print(f\"[ERRO CRÍTICO] Não foi possível obter a SparkSession: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Teste Direto do ContextManager\n",
                "Instanciamos a classe e chamamos o método principal. Se isso falhar, o erro está dentro da função."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from app.context_manager import ContextManager\n",
                "\n",
                "try:\n",
                "    print(\"Inicializando ContextManager...\")\n",
                "    context_mgr = ContextManager(spark)\n",
                "    \n",
                "    role = \"logistics\"\n",
                "    print(f\"Gerando schema para o papel: '{role}' (esperado: olist_sales e olist_logistics)...\")\n",
                "    \n",
                "    context_output = context_mgr.get_schema_context(role)\n",
                "    \n",
                "    if context_output:\n",
                "        print(\"\\n[SUCESSO] Contexto Gerado com Sucesso!\")\n",
                "        print(\"=\"*40)\n",
                "        print(context_output[:1000]) # Imprime os primeiros 1000 caracteres\n",
                "        print(\"...(truncado)\")\n",
                "        print(\"=\"*40)\n",
                "    else:\n",
                "        print(\"\\n[AVISO] O contexto retornou vazio. As queries funcionaram mas não encontraram tabelas/colunas.\")\n",
                "\n",
                "except Exception as e:\n",
                "    print(f\"\\n[FALHA] Erro ao executar ContextManager: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Debug Granular (Simulação Manual)\n",
                "Se o passo 2 falhou, executamos aqui **linha a linha** o que o código faz internamente.\n",
                "Isso nos diz exatamente onde está o problema: na listagem de tabelas, na conversão pandas, ou no describe."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "domains = [\"olist_dataset.olist_sales\", \"olist_dataset.olist_logistics\"]\n",
                "\n",
                "print(\"Iniciando Debug Granular Manual...\\n\")\n",
                "\n",
                "for domain in domains:\n",
                "    print(f\"--- Analisando Domínio: {domain} ---\")\n",
                "    \n",
                "    # Passo A: Listar Tabelas\n",
                "    try:\n",
                "        print(f\"1. Executando: SHOW TABLES IN {domain}\")\n",
                "        df_tables = spark.sql(f\"SHOW TABLES IN {domain}\").limit(20) # Limitando como no código\n",
                "        \n",
                "        print(\"2. Convertendo tabelas para Pandas...\")\n",
                "        pdf_tables = df_tables.toPandas()\n",
                "        print(f\"   > Sucesso! Encontradas {len(pdf_tables)} tabelas.\")\n",
                "        \n",
                "        # Passo B: Iterar e Descrever\n",
                "        for index, row in pdf_tables.iterrows():\n",
                "            t_name = row['tableName']\n",
                "            full_table = f\"{domain}.{t_name}\"\n",
                "            print(f\"   > Inspecionando tabela: {full_table}\")\n",
                "            \n",
                "            try:\n",
                "                print(f\"     - Executando: DESCRIBE {full_table}\")\n",
                "                df_cols = spark.sql(f\"DESCRIBE {full_table}\")\n",
                "                \n",
                "                print(\"     - Convertendo colunas para Pandas...\")\n",
                "                pdf_cols = df_cols.toPandas()\n",
                "                print(f\"     - OK! {len(pdf_cols)} colunas recuperadas.\")\n",
                "                \n",
                "                # Validação de formato (apenas amostra)\n",
                "                first_col = pdf_cols.iloc[0]\n",
                "                print(f\"     - Exemplo de coluna: {first_col['col_name']} ({first_col['data_type']})\")\n",
                "                \n",
                "            except Exception as e_desc:\n",
                "                print(f\"     [ERRO] Falha ao descrever tabela {full_table}: {e_desc}\")\n",
                "                \n",
                "    except Exception as e_table:\n",
                "        print(f\"[ERRO] Falha crítica no domínio {domain}: {e_table}\")\n",
                "    \n",
                "    print(\"\\n\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}