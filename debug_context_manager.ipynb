{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f5cd45a-2665-4ed7-8cf8-55ead893cd22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Debug Dedicado: Context Manager\n",
    "\n",
    "Este notebook foi criado para isolar e debugar falhas no `ContextManager`, focando em:\n",
    "1. **Conectividade**: Validar se conseguimos listar tabelas e colunas via Spark.\n",
    "2. **Segurança/Permissão**: Verificar se métodos como `.toPandas()` funcionam sem erros de bloqueio (WhiteList/Socket).\n",
    "3. **Formatação de Prompt**: Garantir que a string de contexto gerada esteja correta e legível para o LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea987fec-37e2-401d-91ce-bcae5bc5b071",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Adiciona o diretório atual ao path para importação dos módulos\n",
    "sys.path.append(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fce89ae-2698-42cc-8a33-cb4eb9a6b776",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Inicialização da Sessão Spark (Segura)\n",
    "Tentamos pegar a sessão ativa para evitar conflitos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1130d532-b4be-4cf3-b787-d67bc5eb403e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    spark = SparkSession.getActiveSession()\n",
    "    if not spark:\n",
    "        print(\"Criando nova sessão Spark (Ambiente Local)...\")\n",
    "        spark = SparkSession.builder.appName(\"DebugContextManager\").getOrCreate()\n",
    "    print(f\"[OK] Spark Session: {spark}\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERRO CRÍTICO] Não foi possível obter a SparkSession: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d40d4ec3-ac4f-4893-87ef-8c8317af7d48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Teste Direto do ContextManager\n",
    "Instanciamos a classe e chamamos o método principal. Se isso falhar, o erro está dentro da função."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4a82aae-9fcf-41ef-8b1d-ee4efab5ba44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from app.context_manager import ContextManager\n",
    "\n",
    "try:\n",
    "    print(\"Inicializando ContextManager...\")\n",
    "    context_mgr = ContextManager(spark)\n",
    "    \n",
    "    role = \"logistics\"\n",
    "    print(f\"Gerando schema para o papel: '{role}' (esperado: olist_sales e olist_logistics)...\")\n",
    "    \n",
    "    context_output = context_mgr.get_schema_context(role)\n",
    "    \n",
    "    if context_output:\n",
    "        print(\"\\n[SUCESSO] Contexto Gerado com Sucesso!\")\n",
    "        print(\"=\"*40)\n",
    "        print(context_output[:1000]) # Imprime os primeiros 1000 caracteres\n",
    "        print(\"...(truncado)\")\n",
    "        print(\"=\"*40)\n",
    "    else:\n",
    "        print(\"\\n[AVISO] O contexto retornou vazio. As queries funcionaram mas não encontraram tabelas/colunas.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n[FALHA] Erro ao executar ContextManager: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "146a8674-5b3e-44cd-9174-7b9ca35c8a9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Debug Granular (Simulação Manual)\n",
    "Se o passo 2 falhou, executamos aqui **linha a linha** o que o código faz internamente.\n",
    "Isso nos diz exatamente onde está o problema: na listagem de tabelas, na conversão pandas, ou no describe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e85e463c-c3af-40f5-a729-be79d4d372c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "domains = [\"olist_dataset.olist_sales\", \"olist_dataset.olist_logistics\"]\n",
    "\n",
    "print(\"Iniciando Debug Granular Manual...\\n\")\n",
    "\n",
    "for domain in domains:\n",
    "    print(f\"--- Analisando Domínio: {domain} ---\")\n",
    "    \n",
    "    # Passo A: Listar Tabelas\n",
    "    try:\n",
    "        print(f\"1. Executando: SHOW TABLES IN {domain}\")\n",
    "        df_tables = spark.sql(f\"SHOW TABLES IN {domain}\").limit(20) # Limitando como no código\n",
    "        \n",
    "        print(\"2. Convertendo tabelas para Pandas...\")\n",
    "        pdf_tables = df_tables.toPandas()\n",
    "        print(f\"   > Sucesso! Encontradas {len(pdf_tables)} tabelas.\")\n",
    "        \n",
    "        # Passo B: Iterar e Descrever\n",
    "        for index, row in pdf_tables.iterrows():\n",
    "            t_name = row['tableName']\n",
    "            full_table = f\"{domain}.{t_name}\"\n",
    "            print(f\"   > Inspecionando tabela: {full_table}\")\n",
    "            \n",
    "            try:\n",
    "                print(f\"     - Executando: DESCRIBE {full_table}\")\n",
    "                df_cols = spark.sql(f\"DESCRIBE {full_table}\")\n",
    "                \n",
    "                print(\"     - Convertendo colunas para Pandas...\")\n",
    "                pdf_cols = df_cols.toPandas()\n",
    "                print(f\"     - OK! {len(pdf_cols)} colunas recuperadas.\")\n",
    "                \n",
    "                # Validação de formato (apenas amostra)\n",
    "                first_col = pdf_cols.iloc[0]\n",
    "                print(f\"     - Exemplo de coluna: {first_col['col_name']} ({first_col['data_type']})\")\n",
    "                \n",
    "            except Exception as e_desc:\n",
    "                print(f\"     [ERRO] Falha ao descrever tabela {full_table}: {e_desc}\")\n",
    "                \n",
    "    except Exception as e_table:\n",
    "        print(f\"[ERRO] Falha crítica no domínio {domain}: {e_table}\")\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "debug_context_manager",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
