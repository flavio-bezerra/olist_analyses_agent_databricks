{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "f54c6258-edd8-4304-8965-50a4692eee01",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "source": [
                "# Debug de Componentes do Agente Multi-Camada (Full Stack)\n",
                "\n",
                "Este notebook testa a integração completa.\n",
                "Se o `debug_context_manager.ipynb` funcionou, este também deve funcionar se as importações estiverem atualizadas."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "24a4dd4c-ae8f-411b-adb8-8ac2141ef9d9",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "%load_ext autoreload\n",
                "%autoreload 2\n",
                "import os\n",
                "import sys\n",
                "from pyspark.sql import SparkSession\n",
                "\n",
                "sys.path.append(os.getcwd())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "740e347c-57c8-4b39-9cd8-a8fb5e6b28e6",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "dbutils.notebook.exit(\"Reinicie o notebook manualmente ou execute novamente todas as células.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "ae7b51d2-f581-4497-9321-2ccfddb7ed1d",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "source": [
                "## 1. Inicializando Sessão & Data Engine\n",
                "Verificamos se o Data Engine pega a sessão correta."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "574b33bf-153a-4fc8-8849-5fb8817b0849",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "from app.data_engine import DataEngine\n",
                "\n",
                "print(\"Inicializando DataEngine...\")\n",
                "data_engine = DataEngine()\n",
                "\n",
                "# Pega a sessão (deve ser a mesma do debug anterior)\n",
                "spark = data_engine.initialize_session()\n",
                "print(f\"Spark Session obtida: {spark}\")\n",
                "\n",
                "# Validação cruzada (Sanity Check)\n",
                "active = SparkSession.getActiveSession()\n",
                "if spark == active:\n",
                "    print(\"[OK] DataEngine está usando a sessão ativa correta.\")\n",
                "else:\n",
                "    print(\"[ALERTA] DataEngine criou ou pegou uma sessão diferente da ativa!\")\n",
                "    # Force usage of active if they differ to avoid issues\n",
                "    spark = active"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "e3bb0009-1288-410f-8252-6aa84b0f1239",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "source": [
                "## 2. Teste do Context Manager (Schema Injection)\n",
                "Deve imprimir o schema sem erros de Socket/Security."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "54a90f78-b612-4e84-a9eb-215d1cdaa453",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "from app.context_manager import ContextManager\n",
                "\n",
                "context_mgr = ContextManager(spark)\n",
                "\n",
                "role = \"logistics\"\n",
                "print(f\"Gerando contexto para role: {role}...\")\n",
                "\n",
                "try:\n",
                "    schema_context = context_mgr.get_schema_context(role)\n",
                "    \n",
                "    if schema_context:\n",
                "        print(\"\\n[SUCESSO] Contexto gerado:\")\n",
                "        print(schema_context[:500] + \"... (truncado)\")\n",
                "    else:\n",
                "        print(\"[AVISO] Contexto vazio.\")\n",
                "except Exception as e:\n",
                "    print(f\"[ERRO] Falha no Context Manager dentro do Debug Full: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "5f18b74c-39de-4199-b1d5-6a098b96fc92",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "source": [
                "## 3. Teste do Agente 'Logistics' Isolado (LLM + Tool)\n",
                "Aqui testamos se o LLM consegue receber esse contexto e formular uma query."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "f2b0a3ae-1d68-4f82-9e16-f53419e504a6",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "from app.agents import Agent\n",
                "from app.tools import SparkSQLTool\n",
                "\n",
                "tool = SparkSQLTool(spark)\n",
                "logistics_agent = Agent(\"LogisticsDebug\", \"logistics\", context_mgr, tool)\n",
                "\n",
                "print(\"Executando Agente de Logística...\")\n",
                "# Prompt simples de verificação\n",
                "try:\n",
                "    response = logistics_agent.run(\"Conte quantas linhas existem na tabela orders (olist_dataset.olist_sales.orders).\")\n",
                "    print(\"\\n=== Resposta Final ===\")\n",
                "    print(response)\n",
                "except Exception as e:\n",
                "    print(f\"[ERRO] Falha na execução do Agente: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "9e2486bb-3f3a-4a5c-9367-8b81128a7c5a",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "source": [
                "## 4. Teste do Agente 'Finance' Isolado"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "a060b11a-9296-40ca-8f6b-c4716a1f976b",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "finance_agent = Agent(\"FinanceDebug\", \"finance\", context_mgr, tool)\n",
                "\n",
                "print(\"Executando Agente de Finanças...\")\n",
                "response_fin = finance_agent.run(\"Qual o total de pagamentos registrados na tabela de pagamentos?\")\n",
                "\n",
                "print(\"\\n=== Resposta Final ===\")\n",
                "print(response_fin)"
            ]
        }
    ],
    "metadata": {
        "application/vnd.databricks.v1+notebook": {
            "computePreferences": {
                "hardware": {
                    "accelerator": null,
                    "gpuPoolId": null,
                    "memory": null
                }
            },
            "dashboards": [],
            "environmentMetadata": {
                "base_environment": "",
                "dependencies": [
                    "-r /Workspace/Users/flaviomenegueco@gmail.com/olist_analyses_agent_databricks/requirements.txt"
                ],
                "environment_version": "2"
            },
            "inputWidgetPreferences": null,
            "language": "python",
            "notebookMetadata": {
                "pythonIndentUnit": 4
            },
            "notebookName": "debug_agente",
            "widgets": {}
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}