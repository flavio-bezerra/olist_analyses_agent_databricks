{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Setup Data Mesh - Ingestão e Criação de Tabelas\n",
                "\n",
                "Este notebook é responsável por:\n",
                "1. Inicializar a Sessão Spark.\n",
                "2. Criar o Catálogo `olist_dataset` (se suportado/Unity Catalog).\n",
                "3. Criar os Schemas (Bancos de Dados Lógicos) para cada domínio dentro do catálogo.\n",
                "4. Ler os arquivos CSV brutos.\n",
                "5. Salvar as tabelas no formato gerenciado (Parquet/Delta) com `overwrite`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%load_ext autoreload\n",
                "%autoreload 2\n",
                "\n",
                "import os\n",
                "import sys\n",
                "from pyspark.sql import SparkSession\n",
                "\n",
                "# Garante que o diretório app seja visível\n",
                "sys.path.append(os.getcwd())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Inicialização da Sessão Spark\n",
                "spark = SparkSession.builder \\\n",
                "    .appName(\"SetupDataMesh\") \\\n",
                "    .enableHiveSupport() \\\n",
                "    .getOrCreate()\n",
                "\n",
                "print(\"Spark Session Ativa\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuração dos Caminhos\n",
                "DATA_PATH = \"data/\"\n",
                "\n",
                "# Definição dos Domínios e Arquivos\n",
                "DOMAINS = {\n",
                "    \"olist_sales\": [\n",
                "        \"olist_orders_dataset.csv\", \n",
                "        \"olist_order_items_dataset.csv\", \n",
                "        \"olist_products_dataset.csv\"\n",
                "    ],\n",
                "    \"olist_logistics\": [\n",
                "        \"olist_sellers_dataset.csv\", \n",
                "        \"olist_geolocation_dataset.csv\", \n",
                "        \"olist_customers_dataset.csv\"\n",
                "    ],\n",
                "    \"olist_finance\": [\n",
                "        \"olist_order_payments_dataset.csv\"\n",
                "    ],\n",
                "    \"olist_cx\": [\n",
                "        \"olist_order_reviews_dataset.csv\"\n",
                "    ]\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def ingest_domain(domain_name, file_list):\n",
                "    print(f\"\\n>>> Configurando Domínio: {domain_name}\")\n",
                "    \n",
                "    # 0. Criar Catálogo\n",
                "    # Nota: CREATE CATALOG geralmente requer o Unity Catalog no Databricks.\n",
                "    # Se rodar localmente sem UC, isso pode falhar ou precisa ser adaptado.\n",
                "    try:\n",
                "        print(\"Creating Catalog olist_dataset...\")\n",
                "        spark.sql(\"CREATE CATALOG IF NOT EXISTS olist_dataset\")\n",
                "    except Exception as e:\n",
                "        print(f\"Warning: Could not create catalog (might be local spark). Error: {e}\")\n",
                "        print(\"Creating database in default catalog instead...\")\n",
                "    \n",
                "    # 1. Tentar definir o nome do banco com catálogo\n",
                "    target_db = f\"olist_dataset.{domain_name}\"\n",
                "    try:\n",
                "        print(f\"Creating Schema {target_db}...\")\n",
                "        spark.sql(f\"CREATE DATABASE IF NOT EXISTS {target_db}\")\n",
                "    except:\n",
                "        target_db = domain_name\n",
                "        print(f\"Fallback: Creating Schema {target_db} (default catalog)...\")\n",
                "        spark.sql(f\"CREATE DATABASE IF NOT EXISTS {target_db}\")\n",
                "    \n",
                "    # 2. Iterar arquivos e criar tabelas\n",
                "    for filename in file_list:\n",
                "        file_path = os.path.join(DATA_PATH, filename)\n",
                "        \n",
                "        if not os.path.exists(file_path):\n",
                "            print(f\"  [SKIP] Arquivo não encontrado: {file_path}\")\n",
                "            continue\n",
                "            \n",
                "        # Nome da tabela limpo\n",
                "        table_name = filename.replace(\"olist_\", \"\").replace(\"_dataset.csv\", \"\")\n",
                "        full_table_name = f\"{target_db}.{table_name}\"\n",
                "        \n",
                "        print(f\"  Ingestão: {filename} -> {full_table_name}\")\n",
                "        \n",
                "        # Leitura do CSV\n",
                "        try:\n",
                "            df = spark.read.option(\"header\", \"true\") \\\n",
                "                           .option(\"inferSchema\", \"true\") \\\n",
                "                           .csv(file_path)\n",
                "            \n",
                "            # Escrita com Overwrite (SaveAsTable)\n",
                "            df.write.mode(\"overwrite\").saveAsTable(full_table_name)\n",
                "            print(f\"  [OK] Tabela {full_table_name} atualizada com sucesso.\")\n",
                "            \n",
                "        except Exception as e:\n",
                "            print(f\"  [ERRO] Falha ao processar {filename}: {str(e)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Execução Geral\n",
                "for domain, files in DOMAINS.items():\n",
                "    ingest_domain(domain, files)\n",
                "    \n",
                "print(\"\\n=== Data Mesh Setup Concluído ===\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Validação: Listar tabelas criadas\n",
                "# Tenta listar do catálogo especifico ou do default\n",
                "for domain in DOMAINS.keys():\n",
                "    target_db = f\"olist_dataset.{domain}\"\n",
                "    print(f\"\\nTabelas em {target_db} (ou fallback {domain}):\")\n",
                "    try:\n",
                "        spark.sql(f\"SHOW TABLES IN {target_db}\").show(truncate=False)\n",
                "    except:\n",
                "        spark.sql(f\"SHOW TABLES IN {domain}\").show(truncate=False)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}