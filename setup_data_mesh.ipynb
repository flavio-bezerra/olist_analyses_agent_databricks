{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Setup Data Mesh - Ingestão e Criação de Tabelas\n",
                "\n",
                "Este notebook é responsável por:\n",
                "1. Inicializar a Sessão Spark.\n",
                "2. Criar os Bancos de Dados Lógicos (Schemas) para cada domínio.\n",
                "3. Ler os arquivos CSV brutos.\n",
                "4. Salvar as tabelas no formato gerenciado (Parquet/Delta) com `overwrite`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%load_ext autoreload\n",
                "%autoreload 2\n",
                "\n",
                "import os\n",
                "import sys\n",
                "from pyspark.sql import SparkSession\n",
                "\n",
                "# Garante que o diretório app seja visível\n",
                "sys.path.append(os.getcwd())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Inicialização da Sessão Spark\n",
                "# Se estiver rodando localmente, isso cria um warehouse local.\n",
                "# Se estiver no Databricks, usa a sessão ativa.\n",
                "spark = SparkSession.builder \\\n",
                "    .appName(\"SetupDataMesh\") \\\n",
                "    .enableHiveSupport() \\\n",
                "    .getOrCreate()\n",
                "\n",
                "print(\"Spark Session Ativa\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuração dos Caminhos\n",
                "DATA_PATH = \"data/\"\n",
                "\n",
                "# Definição dos Domínios e Arquivos\n",
                "DOMAINS = {\n",
                "    \"olist_sales\": [\n",
                "        \"olist_orders_dataset.csv\", \n",
                "        \"olist_order_items_dataset.csv\", \n",
                "        \"olist_products_dataset.csv\"\n",
                "    ],\n",
                "    \"olist_logistics\": [\n",
                "        \"olist_sellers_dataset.csv\", \n",
                "        \"olist_geolocation_dataset.csv\", \n",
                "        \"olist_customers_dataset.csv\"\n",
                "    ],\n",
                "    \"olist_finance\": [\n",
                "        \"olist_order_payments_dataset.csv\"\n",
                "    ],\n",
                "    \"olist_cx\": [\n",
                "        \"olist_order_reviews_dataset.csv\"\n",
                "    ]\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def ingest_domain(domain_name, file_list):\n",
                "    print(f\"\\n>>> Configurando Domínio: {domain_name}\")\n",
                "    \n",
                "    # 1. Criar Schema/Database\n",
                "    print(f\"Creating Schema {domain_name}...\")\n",
                "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {domain_name}\")\n",
                "    \n",
                "    # 2. Iterar arquivos e criar tabelas\n",
                "    for filename in file_list:\n",
                "        file_path = os.path.join(DATA_PATH, filename)\n",
                "        \n",
                "        if not os.path.exists(file_path):\n",
                "            print(f\"  [SKIP] Arquivo não encontrado: {file_path}\")\n",
                "            continue\n",
                "            \n",
                "        # Nome da tabela limpo\n",
                "        table_name = filename.replace(\"olist_\", \"\").replace(\"_dataset.csv\", \"\")\n",
                "        full_table_name = f\"{domain_name}.{table_name}\"\n",
                "        \n",
                "        print(f\"  Ingestão: {filename} -> {full_table_name}\")\n",
                "        \n",
                "        # Leitura do CSV\n",
                "        try:\n",
                "            df = spark.read.option(\"header\", \"true\") \\\n",
                "                           .option(\"inferSchema\", \"true\") \\\n",
                "                           .csv(file_path)\n",
                "            \n",
                "            # Escrita com Overwrite (SaveAsTable)\n",
                "            # Isso cria a tabela no Metastore e move os dados para o Warehouse\n",
                "            df.write.mode(\"overwrite\").saveAsTable(full_table_name)\n",
                "            print(f\"  [OK] Tabela {full_table_name} atualizada com sucesso.\")\n",
                "            \n",
                "        except Exception as e:\n",
                "            print(f\"  [ERRO] Falha ao processar {filename}: {str(e)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Execução Geral\n",
                "for domain, files in DOMAINS.items():\n",
                "    ingest_domain(domain, files)\n",
                "    \n",
                "print(\"\\n=== Data Mesh Setup Concluído ===\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Validação: Listar tabelas criadas\n",
                "for domain in DOMAINS.keys():\n",
                "    print(f\"\\nTabelas em {domain}:\")\n",
                "    spark.sql(f\"SHOW TABLES IN {domain}\").show(truncate=False)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}