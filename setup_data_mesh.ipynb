{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8af67c55-956c-4cce-bdcc-08e47d5c5f9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Setup Data Mesh - Ingestão e Criação de Tabelas\n",
    "\n",
    "Este notebook é responsável por:\n",
    "1. Inicializar a Sessão Spark.\n",
    "2. Criar o Catálogo `olist_dataset` (se suportado/Unity Catalog).\n",
    "3. Criar os Schemas (Bancos de Dados Lógicos) para cada domínio dentro do catálogo.\n",
    "4. Ler os arquivos CSV brutos.\n",
    "5. Salvar as tabelas no formato gerenciado (Parquet/Delta) com `overwrite`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15c1e163-c3b4-4ef0-8973-45da85179a5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Garante que o diretório app seja visível\n",
    "sys.path.append(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36933d1b-70ab-4bdd-b6d5-6e7acb76d8f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Inicialização da Sessão Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SetupDataMesh\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Session Ativa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a53ca267-e987-4c4a-82da-54b50fcd3cd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuração dos Caminhos\n",
    "DATA_PATH = \"data/\"\n",
    "\n",
    "# Definição dos Domínios e Arquivos\n",
    "DOMAINS = {\n",
    "    \"olist_sales\": [\n",
    "        \"olist_orders_dataset.csv\", \n",
    "        \"olist_order_items_dataset.csv\", \n",
    "        \"olist_products_dataset.csv\"\n",
    "    ],\n",
    "    \"olist_logistics\": [\n",
    "        \"olist_sellers_dataset.csv\", \n",
    "        \"olist_geolocation_dataset.csv\", \n",
    "        \"olist_customers_dataset.csv\"\n",
    "    ],\n",
    "    \"olist_finance\": [\n",
    "        \"olist_order_payments_dataset.csv\"\n",
    "    ],\n",
    "    \"olist_cx\": [\n",
    "        \"olist_order_reviews_dataset.csv\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc6b3332-bc8c-4b6d-be99-d0f47c6e9844",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " f\"/dbfs/{os.path.join(DATA_PATH, filename)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51c6e19c-9e83-4e0d-a67f-e33b06dc47fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aecb2241-1de7-4a1b-bcd8-d6636a275e6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "649c824d-2b5e-4e10-b423-1f5db0303b41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "/Workspace/Users/flaviomenegueco@gmail.com/olist_analyses_agent_databricks/data/olist_geolocation_dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beebedf3-2cf0-4707-b333-325046dc07d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def ingest_domain(domain_name, file_list):\n",
    "    print(f\"\\n>>> Configurando Domínio: {domain_name}\")\n",
    "    \n",
    "    # Exemplo de caminho dos dados:\n",
    "    print(\"Exemplo de caminho dos dados:\", \"/Workspace/Users/flaviomenegueco@gmail.com/olist_analyses_agent_databricks/data/olist_geolocation_dataset.csv\")\n",
    "    \n",
    "    # 0. Criar Catálogo\n",
    "    try:\n",
    "        print(\"Creating Catalog olist_dataset...\")\n",
    "        spark.sql(\"CREATE CATALOG IF NOT EXISTS olist_dataset\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not create catalog (might be local spark). Error: {e}\")\n",
    "        print(\"Creating database in default catalog instead...\")\n",
    "    \n",
    "    # 1. Tentar definir o nome do banco com catálogo\n",
    "    target_db = f\"olist_dataset.{domain_name}\"\n",
    "    try:\n",
    "        print(f\"Creating Schema {target_db}...\")\n",
    "        spark.sql(f\"CREATE DATABASE IF NOT EXISTS {target_db}\")\n",
    "    except:\n",
    "        target_db = domain_name\n",
    "        print(f\"Fallback: Creating Schema {target_db} (default catalog)...\")\n",
    "        spark.sql(f\"CREATE DATABASE IF NOT EXISTS {target_db}\")\n",
    "    \n",
    "    # 2. Iterar arquivos e criar tabelas\n",
    "    for filename in file_list:\n",
    "        # Ajusta o caminho para Workspace\n",
    "        file_path = f\"/Workspace/Users/flaviomenegueco@gmail.com/olist_analyses_agent_databricks/data/{filename}\"\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"  [SKIP] Arquivo não encontrado: {file_path}\")\n",
    "            continue\n",
    "            \n",
    "        # Nome da tabela limpo\n",
    "        table_name = filename.replace(\"olist_\", \"\").replace(\"_dataset.csv\", \"\")\n",
    "        full_table_name = f\"{target_db}.{table_name}\"\n",
    "        \n",
    "        print(f\"  Ingestão: {filename} -> {full_table_name}\")\n",
    "        \n",
    "        # Leitura do CSV\n",
    "        try:\n",
    "            df = spark.read.option(\"header\", \"true\") \\\n",
    "                           .option(\"inferSchema\", \"true\") \\\n",
    "                           .csv(file_path)\n",
    "            \n",
    "            # Escrita com Overwrite (SaveAsTable)\n",
    "            df.write.mode(\"overwrite\").saveAsTable(full_table_name)\n",
    "            print(f\"  [OK] Tabela {full_table_name} atualizada com sucesso.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  [ERRO] Falha ao processar {filename}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6116a993-cc07-4c83-9efe-0f9fca6370e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Execução Geral\n",
    "for domain, files in DOMAINS.items():\n",
    "    ingest_domain(domain, files)\n",
    "    \n",
    "print(\"\\n=== Data Mesh Setup Concluído ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2820f795-3542-44be-92b0-55936aedc772",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Validação: Listar tabelas criadas\n",
    "# Tenta listar do catálogo especifico ou do default\n",
    "for domain in DOMAINS.keys():\n",
    "    target_db = f\"olist_dataset.{domain}\"\n",
    "    print(f\"\\nTabelas em {target_db} (ou fallback {domain}):\")\n",
    "    try:\n",
    "        spark.sql(f\"SHOW TABLES IN {target_db}\").show(truncate=False)\n",
    "    except:\n",
    "        spark.sql(f\"SHOW TABLES IN {domain}\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "setup_data_mesh",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
