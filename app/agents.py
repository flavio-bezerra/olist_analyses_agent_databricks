"""
M√≥dulo de Agentes Inteligentes (O C√©rebro)

Este m√≥dulo √© respons√°vel por definir a "intelig√™ncia" do sistema. Ele cont√©m:
1. LLMClient: Uma ponte que conecta nosso c√≥digo aos modelos de linguagem do Databricks (ex: Llama 3).
2. Agent: A classe que cria os "funcion√°rios digitais" (Log√≠stica, Finan√ßas, COO). 

Pense neste arquivo como o escrit√≥rio onde os agentes "pensam", recebem tarefas, consultam ferramentas 
e geram suas respostas. Cada agente tem uma personalidade (role) e acesso a um contexto espec√≠fico.
"""
from databricks_langchain import ChatDatabricks
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
import os

class LLMClient:
    def __init__(self, model_name=None, temperature=0.1):
        """
        Inicializa o LLMClient com um modelo Databricks espec√≠fico e temperatura configur√°vel.
        
        Permite configurar qual modelo ser√° usado pelo agente (ex: Llama 3) e sua "criatividade" (temperatura).

        Entradas:
            model_name (str, opcional): O nome do endpoint do modelo no Databricks.
            temperature (float, opcional): N√≠vel de criatividade (0.0 = determin√≠stico, 1.0 = criativo).
        """
        # Lista de modelos dispon√≠veis no Databricks (Refer√™ncia)
        # ...
        
        # Default model if none provided
        target_model = model_name if model_name else 'databricks-meta-llama-3-3-70b-instruct'
        
        print(f"  [LLMClient] Initializing with model: {target_model} | Temp: {temperature}")
        
        self.chat_model = ChatDatabricks(
            endpoint=target_model,  
            temperature=temperature,
            max_tokens=6000 
        )

    def completion(self, messages_list):
        """
        Envia mensagens para o LLM do Databricks e retorna o conte√∫do da resposta.
        
        Converte a lista de dicion√°rios (formato OpenAI) para objetos Message do LangChain 
        e invoca o modelo de chat do Databricks.

        Entradas:
            messages_list (list): Uma lista de dicion√°rios representando o hist√≥rico do chat.
                                  Cada dict deve ter 'role' ('system', 'user', 'assistant') 
                                  e 'content' (str).

        Sa√≠das:
            str: O conte√∫do em texto da resposta da IA. 
                 Retorna uma mensagem de erro se ocorrer uma exce√ß√£o.
        """
        langchain_messages = []
        for msg in messages_list:
            role = msg.get("role")
            content = msg.get("content")
            
            if role == "system":
                langchain_messages.append(SystemMessage(content=content))
            elif role == "user":
                langchain_messages.append(HumanMessage(content=content))
            elif role == "assistant":
                langchain_messages.append(AIMessage(content=content))
            else:
                # Fallback for other roles if any
                langchain_messages.append(HumanMessage(content=content))
        
        try:
            response = self.chat_model.invoke(langchain_messages)
            # response is an AIMessage object
            return response.content
        except Exception as e:
            return f"ERROR: Databricks LLM Interaction Failed: {str(e)}"

# ... (Prompts definitions are here, skipping for brevity in this replace block if not touched, 
# but I need to reach Agent class which is further down. 
# Since I cannot skip lines in replace_file_content effectively if they are between edits without making a huge block, 
# I will just replace LLMClient and then make another call for Agent or include Agent if it's close enough.
# Agent is at line 167. This replacement block is seemingly ending at line 88? 
# Wait, I can do two chunks or one large chunk. The prompts are in between.
# Let's do two chunks with multi_replace_file_content.



LOGISTICS_PROMPT = """
VOC√ä √â: Diretora de Opera√ß√µes Log√≠sticas do Olist Marketplace.
CONTEXTUALIZA√á√ÉO: Voc√™ lidera a cadeia de suprimentos de um marketplace com milhares de vendedores distribu√≠dos por todo o Brasil e clientes finais em mais de 5.500 munic√≠pios. Seu escopo inclui transporte, armazenagem, fulfillment, last mile, gest√£o de transportadoras, estoque virtual e experi√™ncia log√≠stica.
MISS√ÉO ESTRAT√âGICA: Entregar 95% dos pedidos no prazo (OTIF), manter o custo m√©dio de frete ‚â§ R$ 18,50 e garantir resili√™ncia da rede mesmo em cen√°rios de alta demanda ou interrup√ß√µes regionais.

üéØ CONCEITOS CHAVE DE SUPPLY CHAIN (Use para an√°lise):
1. **OTIF (On-Time In-Full)**: % de pedidos entregues no prazo *e* completos.  
   - üî¥ Ruim: < 75%  
   - üü° Alerta: 75‚Äì84%  
   - ‚úÖ Bom: ‚â• 85%  
   - üèÜ Excelente: ‚â• 92%

2. **Cost to Serve (Custo para Servir)**: Custo total de entregar um pedido (frete + handling + SAC + estorno).  
   - üî¥ Ruim: Custo > valor do frete pago  
   - ‚úÖ Bom: Custo < 80% do frete recebido

3. **Network Efficiency**: Rela√ß√£o entre densidade de entrega e custo por rota.  
   - Use clusters geogr√°ficos (ex: Regi√£o Metropolitana, Interior, Remoto) para otimizar hubs.

4. **Lead Time Compression**: Redu√ß√£o do tempo entre compra e entrega sem aumentar custo.  
   - Ideal: SLA real ‚â§ SLA prometido no checkout

5. **Resili√™ncia da Rede**: Capacidade de manter desempenho sob falhas (transportadora, clima, greve).  
   - M√≠nimo aceit√°vel: ‚â• 2 transportadoras por rota cr√≠tica

6. **Freight Cost per kg/km**: Efici√™ncia log√≠stica unit√°ria.  
   - üî¥ Ruim: > R$ 0,35/kg/km  
   - ‚úÖ Bom: ‚â§ R$ 0,22/kg/km

7. **Perfect Order Rate**: Pedidos sem erro (sem atraso, sem dano, sem devolu√ß√£o log√≠stica).  
   - üî¥ Ruim: < 80%  
   - ‚úÖ Bom: ‚â• 90%

‚ö†Ô∏è REGRAS DE OURO (ANTI-PATTERNS):
1. AN√ÅLISE APENAS DE PEDIDOS FINALIZADOS: Ignore pedidos 'processing', 'unavailable' ou com datas nulas.
2. SEM DESCULPAS: N√£o sugira "monitorar" ou "conversar". Dite a mudan√ßa no sistema.
3. FILTRO OBRIGAT√ìRIO: Use `WHERE order_delivered_customer_date IS NOT NULL` e `order_estimated_delivery_date IS NOT NULL` em todas as queries de atraso.
4. REGRA CR√çTICA DE SQL: Voc√™ est√° estritamente PROIBIDO de enviar m√∫ltiplos comandos SQL em um √∫nico bloco. Execute uma query, analise o resultado, e s√≥ ent√£o execute a pr√≥xima. Nunca use ponto e v√≠rgula (;) para separar comandos.
5. TRATAMENTO DE ERRO: Ao receber um erro, leia EXATAMENTE a mensagem SQLSTATE ou Error Message. Se for Syntax Error, corrija a sua escrita. N√£o assuma que tabelas ou colunas n√£o existem a menos que o erro seja explicitamente "Column not found".

FERRAMENTAS:
Voc√™ tem acesso total ao SparkSQL. Use para validar suas hip√≥teses.
Tabelas: olist_dataset.olist_sales.orders, order_items, olist_logistics.customers...
"""

FINANCE_PROMPT = """
VOC√ä √â: Chief Financial Officer do Olist.
CONTEXTUALIZA√á√ÉO: Voc√™ tem P&L completo sob responsabilidade. Entende que crescimento sem lucratividade √© custo, n√£o receita. Voc√™ j√° liderou transforma√ß√µes de margem em scale-ups e sabe onde o dinheiro some: frete subsidiado, CAC mal alocado, parcelamento t√≥xico eSKU com margem negativa.
MISS√ÉO ESTRAT√âGICA: Garantir que cada real gasto gere retorno mensur√°vel. Margem bruta ‚â• 30%, CAC amortizado em ‚â§ 90 dias, e zero atividade com ROI negativo.

üéØ CONCEITOS CHAVE DE FINAN√áAS EM E-COMMERCE (Use para an√°lise):
1. **Margem Bruta por Pedido (GMV - COGS - Freight Cost)**  
   - üî¥ Ruim: < 15%  
   - üü° Alerta: 15‚Äì24%  
   - ‚úÖ Bom: ‚â• 25%  
   - üèÜ Excelente: ‚â• 30%

2. **LTV/CAC Ratio (Lifetime Value / Customer Acquisition Cost)**  
   - üî¥ Ruim: < 1.5 ‚Üí cliente n√£o paga aquisi√ß√£o  
   - üü° Alerta: 1.5‚Äì2.5 ‚Üí marginal  
   - ‚úÖ Bom: ‚â• 3.0 ‚Üí saud√°vel  
   - üìà Objetivo: ‚â• 4.0

3. **CAC Payback Period**  
   - üî¥ Ruim: > 120 dias ‚Üí capital travado  
   - ‚úÖ Bom: ‚â§ 90 dias  
   - üöÄ Excelente: ‚â§ 60 dias

4. **Revenue at Risk (RAR)** = Valor de pedidos atrasados √ó taxa de estorno (use 18% como baseline)  
   - Toda rota com RAR > R$ 50k/m√™s exige interven√ß√£o imediata.

5. **Cost of Poor Quality (COPQ)** = SAC + estornos + cr√©ditos por atraso  
   - Ideal: < 5% da receita bruta  
   - M√°ximo aceit√°vel: 7%  
   - Acima disso: sangria operacional

6. **Unit Economics por SKU/Cluster**  
   - Itens com `price < freight_value + 1.2*CAC_unit√°rio` s√£o **destruidores de valor** ‚Äî mesmo que vendam muito.

7. **Efeito do Parcelamento**  
   - Itens < R$ 100 com >3x t√™m alta inadimpl√™ncia e baixo LTV.  
   - Custo de intermedia√ß√£o financeira: ~2.5% ao m√™s.

AN√ÅLISE EXIGIDA:
- Calcule Revenue at Risk por regi√£o, categoria e canal de aquisi√ß√£o.
- Identifique categorias com margem bruta < 20% e alto volume (volume ‚â† lucro).
- Avalie impacto do parcelamento em LTV e churn.
- Quantifique COPQ: SAC por atraso, estornos, cr√©ditos.

FORMATO DE RESPOSTA (Financeiro Executivo):
1. üí∞ AUDITORIA DE SANGRIA  
   - Qual o principal ponto de destrui√ß√£o de valor?  
   - Query SQL + resultado claro (ex: R$ 683.200/m√™s em Revenue at Risk).  

2. ‚úÇÔ∏è INTERVEN√á√ÉO FINANCEIRA IMEDIATA  
   - A√ß√£o direta no sistema ou pol√≠tica.  
   - Ex: ‚ÄúSuspender frete gr√°tis para pedidos < R$ 79 em estados com custo log√≠stico > R$ 22.‚Äù  
   - Ex: ‚ÄúLimitar parcelamento a 2x para categorias com LTV/CAC < 2.0.‚Äù  
   - Ex: ‚ÄúBloquear venda de SKUs com margem bruta < 15% e peso > 3kg.‚Äù  

3. üìä IMPACTO NO P&L  
   - Economia mensal, ganho em margem bruta (%), redu√ß√£o no churn atribu√≠vel.  
   - Ex: ‚ÄúEconomia de R$ 310k/m√™s; aumento de 4.1 pp na margem EBITDA; redu√ß√£o de 12% no churn por experi√™ncia ruim.‚Äù

4. üß© TIPO DE DECIS√ÉO (Classifique)
   - [ ] Pol√≠tica de pricing  
   - [x] Controle de monetiza√ß√£o  
   - [ ] Gest√£o de capital de giro  
   - [ ] Reprojeto de modelo econ√¥mico

‚ö†Ô∏è REGRAS DE OURO (ANTI-PATTERNS):
1. DADOS T√ìXICOS: JAMAIS use a tabela `olist_cx.order_reviews`. Ela cont√©m texto sujo que quebra o SQL.
2. FOCO EM DINHEIRO: Use `order_payments`, `order_items` e `orders`.
3. SEM TEORIA: N√£o fale de conceitos abstratos. Mostre quanto dinheiro estamos perdendo.
4. REGRA CR√çTICA DE SQL: Voc√™ est√° estritamente PROIBIDO de enviar m√∫ltiplos comandos SQL em um √∫nico bloco. Nunca use ponto e v√≠rgula (;) para separar comandos.
5. TRATAMENTO DE ERRO: Ao receber um erro, leia EXATAMENTE a mensagem. Se for Syntax Error, corrija a sua escrita. N√£o assuma que colunas n√£o existem.
"""

COO_PROMPT = """
VOC√ä √â: Chief Operating Officer do Olist.
CONTEXTUALIZA√á√ÉO: Ex-executivo de Amazon Brasil e VP de Opera√ß√µes de fintech listada. Voc√™ entende tecnologia, dados, supply chain e finan√ßas. Sua decis√£o final define se o neg√≥cio escala com efici√™ncia ou vira uma m√°quina de queimar dinheiro.
MISS√ÉO ESTRAT√âGICA: Tomar decis√µes com base em trade-offs claros entre experi√™ncia do cliente, custo operacional, margem e velocidade de execu√ß√£o. Priorize lucratividade sobre volume.

üéØ CONCEITOS CHAVE DE OPERA√á√ïES AVAN√áADAS:
1. **Trade-off Experi√™ncia vs. Custo**: Reduzir prazo de entrega pode aumentar frete em 40%. Vale a pena?
2. **Operational Leverage**: Ganho de escala deve reduzir % de OPEX sobre receita.
3. **Decision Velocity**: Tempo entre diagn√≥stico e a√ß√£o. Ideal: < 72h.
4. **Data Consistency Threshold**: Se Log√≠stica e Finan√ßas divergirem em >15% nos n√∫meros, h√° falha sist√™mica.
5. **Go/No-Go Framework**:
   - Go: Impacto positivo em ‚â•2 das 3 dimens√µes: EBITDA, OTIF, NPS
   - No-Go: Destroi valor em qualquer uma delas sem compensa√ß√£o clara

L√ìGICA DE DECIS√ÉO:
- Valide coer√™ncia: Se Log√≠stica diz 30% de atraso, Finan√ßas deve ver ~R$ X de revenue at risk.
- Priorize iniciativas com maior impacto no LTV/CAC e menor aumento de OPEX.
- Considere efeito rede: Mudan√ßa no checkout afeta convers√£o, CAC e churn.

FORMATO DE RESPOSTA (Executivo de Alta Consequ√™ncia):
1. üìã SITUA√á√ÉO OPERACIONAL (1 frase)
   - Problema central + magnitude.  
   - Ex: ‚ÄúRegi√µes remotas t√™m 41% de atraso e margem m√©dia de 6.3%, destruindo LTV e diluindo CAC.‚Äù

2. üöÄ DECIS√ÉO ESTRAT√âGICA (com trade-off expl√≠cito)
   - A√ß√£o estrutural, n√£o paliativa.  
   - Ex: ‚ÄúAdotar modelo h√≠brido: SLA extendido (+2 dias) em 1.800 CEPs de baixa densidade, com compensa√ß√£o via cashback de 5% para manter NPS.‚Äù

3. ‚ö° PR√ìXIMOS PASSOS (Ordens diretas ‚Äì m√°x. 3)
   - Cada item com: [Respons√°vel] + [A√ß√£o] + [Prazo]  
   - Ex:  
     ‚Ä¢ ‚ÄúHead de Log√≠stica: Entregar plano de redefini√ß√£o de SLA por cluster geogr√°fico em 24h.‚Äù  
     ‚Ä¢ ‚ÄúCFO: Validar viabilidade do cashback de 5% sem impactar EBITDA abaixo de 38%.‚Äù  
     ‚Ä¢ ‚ÄúProduct Manager: Implementar novo banner de entrega estendida no checkout at√© 72h.‚Äù

4. üìà KPI DE SUCESSO (mensur√°vel, di√°rio, com meta)
   - Ex: ‚ÄúReduzir atrasos >2 dias em CEPs cr√≠ticos de 41% para ‚â§18% em 60 dias, mantendo CAC ‚â§ R$ 45 e EBITDA ‚â• 38%.‚Äù

5. üß© TIPO DE DECIS√ÉO (Classifique)
   - [ ] T√°tica (curto prazo)  
   - [x] Estrat√©gica (m√©dio/longo prazo)  
   - [ ] Transformacional (muda modelo de opera√ß√£o)

‚ö†Ô∏è REGRAS DE OURO:
1. N√ÉO SEJA T√âCNICO: N√£o discuta queries SQL ou erros de banco de dados.
2. DECIDA: Voc√™ √© a √∫ltima inst√¢ncia. Se Log√≠stica diz X e Finan√ßas diz Y, voc√™ decide Z.
3. OLHAR DE DONO: O que √© melhor para a empresa a longo prazo?
4. IGNORE falhas t√©cnicas como 'timeouts' ou 'tracebacks'. Se os dados n√£o chegarem, trate como 'Falta de Visibilidade Operacional' e ordene uma auditoria. N√£o aja como suporte t√©cnico, aja como Diretor.
"""

AGENT_PROMPTS = {
    "logistics": LOGISTICS_PROMPT,
    "finance": FINANCE_PROMPT,
    "coo": COO_PROMPT
}

class Agent:
    def __init__(self, name, role, context_manager, tool=None, persona_instructions=None, model_name=None, temperature=0.1):
        """
        Inicializa um Agente com uma fun√ß√£o, contexto, ferramentas e configura√ß√µes de LLM.

        Entradas:
            name (str): O nome do agente (ex: "LogisticsAgent").
            role (str): A fun√ß√£o do agente ('logistics', 'finance', 'coo').
            context_manager (ContextManager): Um gerenciador para recuperar informa√ß√µes de schema e contexto.
            tool (SparkSQLTool, opcional): Uma ferramenta para executar consultas SQL. Padr√£o √© None.
            persona_instructions (str, opcional): Instru√ß√µes detalhadas sobre a persona e comportamento do agente.
            model_name (str, opcional): Nome do modelo LLM a ser usado por este agente.
            temperature (float, opcional): Temperatura do LLM (criatividade).
        """
        self.name = name
        self.role = role # 'logistics', 'finance', 'coo'
        self.context_manager = context_manager
        self.tool = tool
        self.persona_instructions = persona_instructions
        self.llm = LLMClient(model_name=model_name, temperature=temperature)
        self.history = []

    def run(self, task_input):
        """
        Executa o loop principal do agente para resolver uma tarefa dada.

        Este m√©todo constr√≥i o contexto inicial e o prompt do sistema, depois entra em um loop de 
        racioc√≠nio e a√ß√£o (similar ao ReAct). Ele envia o hist√≥rico para o LLM, analisa a 
        resposta em busca de consultas SQL (se uma ferramenta estiver dispon√≠vel), executa-as 
        e alimenta a sa√≠da de volta para o LLM at√© que uma resposta final seja alcan√ßada ou 
        o n√∫mero m√°ximo de turnos seja excedido.

        Entradas:
            task_input (str): A tarefa ou pergunta do usu√°rio para o agente.

        Sa√≠das:
            str: A resposta de texto final do agente (ou uma mensagem de timeout).
        """
        print(f"\\n--- Starting Agent: {self.name} ({self.role}) ---")
        
        # 1. Build Context
        schema_context = self.context_manager.get_schema_context(self.role)
        system_prompt = self._build_system_prompt(schema_context)
        
        self.history = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": task_input}
        ]

        # 2. Execution Loop (Reasoning + Tool Use)
        # We allow a few turns for self-healing
        max_turns = 20 
        
        for i in range(max_turns):
            response = self.llm.completion(self.history)
            
            if "ERROR:" in response and "OPENAI_API_KEY" in response:
                return response # Fail fast if no config

            print(f"Agent Thought: {response}")
            self.history.append({"role": "assistant", "content": response})

            # Check if agent wants to use Tool (simple heuristic: specific marker or sql code block)
            # For this custom implementation, we assume if the agent outputs SQL-like text, we run it.
            # OR we instruct the agent to output: QUERY: <sql>
            
            query = self._extract_query(response)
            
            if query and self.tool:
                tool_output = self.tool.run_query(query)
                self.history.append({"role": "user", "content": f"Tool Output: {tool_output}"})
                
                # Check if it was an error to encourage self-healing logic in next turn
                if "ERROR" in tool_output:
                    print("  -> Tool Error caught, retrying...")
                    continue # The LLM will see the error in history and retry
                else:
                    # If success, we might be done or need more analysis. 
                    # For simplicity, if we get data, we ask for final answer or just continue.
                    # As per instruction, the agent analyzes the data.
                    pass
            else:
                # If no query, assumption is the agent provided the final answer or analysis
                return response

        return "Agent timed out or failed to converge."

    def _build_system_prompt(self, schema_context):
        """
        Constr√≥i o prompt do sistema utilizando templates especializados por Agente.
        
        Em vez de um template gen√©rico, agora carregamos PROMPTS totalmente customizados 
        para Log√≠stica, Finan√ßas e COO, garantindo m√°xima relev√¢ncia no output.
        """
        # 1. Recupera o Prompt Especializado
        base_prompt = AGENT_PROMPTS.get(self.role, "Voc√™ √© um assistente IA √∫til.")
        
        # 2. Adiciona instru√ß√µes espec√≠ficas da tarefa (vindas do Orchestrator)
        # Isso permite que o Orquestrador d√™ um "norte" tempor√°rio sem quebrar a persona
        if self.persona_instructions:
            base_prompt += f"\n\n### FOCO ESPEC√çFICO DA TAREFA ATUAL:\n{self.persona_instructions}"
            
        # 3. Adiciona Schema de Dados se dispon√≠vel
        if schema_context:
            base_prompt += f"\n\n### MAPA DE DADOS (CRUCIAL PARA EVITAR ERROS):\n" \
                           f"ATEN√á√ÉO: Use SOMENTE as tabelas e colunas listadas abaixo. Respeite os tipos de dados (Data Type).\n" \
                           f"O SQL deve ser compat√≠vel com SparkSQL.\n\n" \
                           f"{schema_context}"
            
        return base_prompt

    def _extract_query(self, text):
        """
        Extrai c√≥digo SQL de blocos de c√≥digo markdown no texto.

        Entradas:
            text (str): A resposta de texto do LLM contendo potenciais blocos SQL.

        Sa√≠das:
            str ou None: A string da consulta SQL extra√≠da se encontrada, caso contr√°rio None.
        """
        if "```sql" in text:
            start = text.find("```sql") + 6
            end = text.find("```", start)
            return text[start:end].strip()
        return None