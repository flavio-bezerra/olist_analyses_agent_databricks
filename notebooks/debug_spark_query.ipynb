{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6769184f-306f-4ace-a39f-c59c4512e392",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Validação de Conectividade e Acesso a Dados (Diagnóstico)\n",
    "\n",
    "Este notebook executa comandos SQL diretos para validar se a sessão Spark consegue acessar o catálogo e as tabelas.\n",
    "Isso ajuda a isolar se o problema é código Python (ContextManager) ou infraestrutura Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ef3bc12-51b6-436c-b9f4-902c1b9700d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Tenta obter a sessão ativa (Databricks) ou criar uma nova se necessário (Local)\n",
    "try:\n",
    "    spark = SparkSession.getActiveSession()\n",
    "    if not spark:\n",
    "        print(\"Nenhuma sessão ativa encontrada. Criando nova...\")\n",
    "        spark = SparkSession.builder.appName(\"Diagnostico\").getOrCreate()\n",
    "    print(f\"Sessão Spark obtida: {spark}\")\n",
    "except Exception as e:\n",
    "    print(f\"[CRITICO] Falha ao obter SparkSession: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9be20bf5-172c-4571-8ca5-f4a337a06c94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- 1. Listando Schemas no Catálogo 'olist_dataset' ---\")\n",
    "try:\n",
    "    spark.sql(\"SHOW SCHEMAS IN olist_dataset\").limit(100).show(truncate=False)\n",
    "except Exception as e:\n",
    "    print(f\"[ERRO] Falha ao listar schemas: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "619bdea1-62d5-49c3-963f-6752d4d86d04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- 2. Listando Tabelas nos Schemas do Catálogo 'olist_dataset' ---\")\n",
    "try:\n",
    "    df_cx = spark.sql(\"SHOW TABLES IN olist_dataset.olist_cx\")\n",
    "    df_finance = spark.sql(\"SHOW TABLES IN olist_dataset.olist_finance\")\n",
    "    df_logistics = spark.sql(\"SHOW TABLES IN olist_dataset.olist_logistics\")\n",
    "    df_sales = spark.sql(\"SHOW TABLES IN olist_dataset.olist_sales\")\n",
    "    df_all = df_cx.unionByName(df_finance).unionByName(df_logistics).unionByName(df_sales)\n",
    "    display(df_all)\n",
    "except Exception as e:\n",
    "    print(f\"[ERRO] Falha ao listar tabelas: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f762dc5-e910-4e95-ba30-37d1a6db48b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- 3. Amostra de Dados de 'olist_dataset.olist_cx.order_reviews' ---\")\n",
    "try:\n",
    "    spark.sql(\"SELECT * FROM olist_dataset.olist_cx.order_reviews\").limit(5).show(truncate=False)\n",
    "except Exception as e:\n",
    "    print(f\"[ERRO] Falha ao ler dados da tabela: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b806e6d6-f920-4633-af84-33d271826dbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- 4. Teste de Coleta para Driver (toPandas) ---\")\n",
    "# Esse teste verifica se há problemas de comunicação Driver <-> Executor (erros de socket)\n",
    "try:\n",
    "    df = spark.sql(\"SELECT * FROM olist_dataset.olist_cx.order_reviews LIMIT 5\")\n",
    "    pdf = df.toPandas()\n",
    "    print(\"Sucesso ao converter para Pandas:\")\n",
    "    print(pdf.head())\n",
    "except Exception as e:\n",
    "    print(f\"[ERRO] Falha ao coletar dados para o Driver (toPandas): {e}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "debug_spark_query",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
